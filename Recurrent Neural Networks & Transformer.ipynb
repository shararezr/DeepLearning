{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch7HKTAoRXBJ"
      },
      "source": [
        "#**Deep Learning Homework 4: *Recurrent Neural Networks & Transformer***\n",
        "### MSc Computer Science, Data Science, Cybersecurity @UNIPD\n",
        "### 2nd semester - 6 ECTS\n",
        "### Prof. Alessandro Sperduti, Prof. Nicol√≤ Navarin and Dr. Luca Pasa\n",
        "---\n",
        "In this homework, we will explore how to develop a simple Recurrent Neural Network (RNN) for sentiment analysis. We will use the IMDB dataset---it contains the text of some reviews and the sentiment given by their authors (either positive or negative). The input to the RNN is the sequence of words that compose a review, so the learning task consists in predicting the overall sentiment of the review.\n",
        "In the first part, we will learn how to develop a simple RNN, then we will explore the differences in terms of computational load, number of parameters, and performances with respect to more advanced recurrent models, like LSTM and GRU. Subsequently, we experiment with the bi-directional model to unveil the strengths and the weaknesses of this technique. Finally, we will solve the same classification problem with a Transformer, in order to have a closer look at its internal functioning.\n",
        "\n",
        "---\n",
        "##**Important Instructions for Submissions:**\n",
        "\n",
        "Generally, in the homeworks, you will be either required to complete a part of Python code or to answer questions in text cells. Code and text cells where you are expected to write your answers have been marked by `%STARTCODE` and `%ENDCODE` or `%STARTEXT` and `%ENDTEXT` tags, respectively. Note that you should never change, move or remove these two tags, otherwise your answers will be __not__ valid. As you will see in this notebook, each cell that includes a `[TO COMPLETE]` part has been put between these placeholders. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu9LiLxTXt_X"
      },
      "source": [
        "#Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agBmeg2KY_KN"
      },
      "outputs": [],
      "source": [
        "!pip3 install datasets skorch pandas~=1.5 torch~=1.13 torchinfo torchdata~=0.5 torchtext~=0.14 torchvision~=0.14 torchaudio~=0.13 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAaDwXybXvnw"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fU8q5qubXwtf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Embedding, RNN, LSTM, GRU, Linear, Transformer\n",
        "import torch.nn.functional as F\n",
        "from torchinfo import summary\n",
        "from torchtext.datasets import IMDB\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchvision.transforms import Lambda\n",
        "from timeit import default_timer as timer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "torch.manual_seed(42)\n",
        "rng = np.random.default_rng(seed=4242)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sysNS560Xxux"
      },
      "source": [
        "#Data Loading and Preprocessing\n",
        "\n",
        "###Load dataset:\n",
        "In this HW, we use the same datset used in the HW2, the IMDB dataset. The dataset contains 50,000 movie reviews from IMDB, labeled by sentiment (positive/negative). As usual, for speed and efficiency, we will use only a subset of the dataset. Reviews have been preprocessed, and each review is encoded as a sequence of word indexes. We load the data from the PyTorch database and then split the data into train, validation and test set. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQpvhGzaJ87l"
      },
      "outputs": [],
      "source": [
        "train, test = IMDB(root=\"dataset\", split=('train', 'test'))\n",
        "Tr, Ts = list(train), list(test)\n",
        "rng.shuffle(Tr)\n",
        "rng.shuffle(Ts)\n",
        "len_train, len_val, len_test = 25000, 12500, 12500 # Whole data\n",
        "train_data, val_data, test_data = Tr[:len_train], Ts[:len_val], Ts[len_val:len_val+len_test]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6PC19R6Z_AQ"
      },
      "outputs": [],
      "source": [
        "len(train_data), len(val_data), len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twbK_FAwZ_Cj"
      },
      "outputs": [],
      "source": [
        "idx = 10\n",
        "label_samp, text_samp = train_data[idx]\n",
        "print(f\"text: {text_samp}\")\n",
        "print(f\"label: {label_samp}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvhHoF4nT94Q"
      },
      "source": [
        "Let's check the dataset statistics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PiJVVkgZ_FJ"
      },
      "outputs": [],
      "source": [
        "datasets = {'train':train_data, 'val':val_data, 'test':test_data}\n",
        "for key in datasets:\n",
        "  label_dist = {}\n",
        "  dataset = datasets[key]\n",
        "  for lb, txt in dataset:\n",
        "    if lb not in label_dist:\n",
        "      label_dist.setdefault(lb, 1)\n",
        "    else:\n",
        "      label_dist[lb] += 1\n",
        "  print(f\"{key}:\")\n",
        "  print(label_dist)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iO3XxTFFZ_J4",
        "outputId": "d210d3f4-221e-4603-ac2e-de3e1d68cc82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Our vocabulary is made of 10000 tokens-index pairs.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "def create_tokens(dataset):\n",
        "  for sample in dataset:\n",
        "    yield tokenizer(sample[1])\n",
        "\n",
        "vocab = build_vocab_from_iterator(create_tokens(train_data), specials=[\"<oov>\", \"<sos>\"], max_tokens=10000)\n",
        "vocab.set_default_index(vocab[\"<oov>\"])\n",
        "print(f\"Our vocabulary is made of {len(vocab)} tokens-index pairs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "octruo_DZ_MW"
      },
      "outputs": [],
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rU-jkLc785iB"
      },
      "outputs": [],
      "source": [
        "print(vocab.get_itos())  # top 10000 freq words (including special chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ohTgQRU6py6"
      },
      "outputs": [],
      "source": [
        "word_index = {k:v for (k, v) in enumerate(vocab.get_itos())}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LgHdz6a6U1g"
      },
      "outputs": [],
      "source": [
        "def decode_review(word_ids):\n",
        "    return \" \".join([word_index.get(word_id, \"<err>\") for word_id in word_ids])\n",
        "\n",
        "print(text_samp)\n",
        "print(\"\\n\")\n",
        "print(decode_review(text_pipeline(text_samp)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHkfIQpAVkWA"
      },
      "source": [
        "To keep the length of the all the input sequences same, we define the padding function. All the sentence less than the lenght of 500 will be padded with zeros and greater than 500 will be truncated. Notice that we will pad and truncate sequences right-wise, so that processing the sequences, the final hidden states of the recurrent networks will correspond to the final words of each sequence in the batch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ng1AGY-MBvLo"
      },
      "outputs": [],
      "source": [
        "def sent_padding(sent_vec, maxlen):\n",
        "  sent_vec = torch.tensor(sent_vec)\n",
        "  maxlen -= len(sent_vec)\n",
        "  return F.pad(sent_vec, (maxlen, 0))\n",
        "\n",
        "\n",
        "print(sent_padding([1,2,3], maxlen=6))\n",
        "print(sent_padding([1,2,3,4,5,6,7,8,9], maxlen=6))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEu2kEw8AitP"
      },
      "outputs": [],
      "source": [
        "seq_len = 500\n",
        "sent_padding(text_pipeline(text_samp), maxlen=seq_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mlDkQCmOCpKY"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kZBk07MCscA"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "  \n",
        "  def __init__(self, dataset, seq_len=seq_len):\n",
        "    super().__init__()\n",
        "    self.dataset = dataset\n",
        "    self.seq_len = seq_len\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.dataset) \n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    label, text = self.dataset[idx]\n",
        "    label = label_pipeline(label)\n",
        "    txt_rep = sent_padding(text_pipeline(text), maxlen=self.seq_len)\n",
        "    label, txt_rep = torch.tensor(label, dtype=torch.float32), torch.tensor(txt_rep, dtype=torch.long)\n",
        "    return label.to(device), txt_rep.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSt_NtvtCse3"
      },
      "outputs": [],
      "source": [
        "train_dataset = CustomDataset(train_data)\n",
        "val_dataset = CustomDataset(val_data)\n",
        "test_dataset = CustomDataset(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LR_4LyNCshb"
      },
      "outputs": [],
      "source": [
        "batch_size=256\n",
        "dataloader_training = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "dataloader_validation = DataLoader(val_dataset, batch_size=batch_size)\n",
        "dataloader_test = DataLoader(test_dataset, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsd_sk1pX04P"
      },
      "source": [
        "#Model Definition\n",
        "\n",
        "Let's define the model: \n",
        "- The first layer is an Embedding layer, with input_dim=vocab_dim and output_dim=10. The model will gradually learn to represent each of the 10,000 words as a 10-dimensional vector. So the next layer will receive 3D batches of shape (batch size, 500, 10)\n",
        "- The second layer is the recurrent one. In particular, in this case, we use a [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html) \n",
        "- The output layer \n",
        "\n",
        "### Model Comparison\n",
        "\n",
        "In order to perform a fair comparison of the models RNN, LSTM and GRU make sure they use more or less the same number of parameters.\n",
        "\n",
        "In the next cell, we define our simple RNN used for binary classification. The class has two main methods, the constructor (`init()`) and the `forward()` method.\n",
        "\n",
        "In the constructor, the input parameters are used to define the layers and hyperparameters of the RNN. The layers that are defined include an embedding layer (`self.embedding`), a recurrent layer (`self.rnn`), and a linear layer (`self.linear`). The constructor also sets up various parameters such as the embedding input size, the embedding output size, the hidden size, the number of layers, the batch size, the RNN type, and whether or not the RNN is bidirectional.\n",
        "\n",
        "The `forward()` method takes a batch of input data (`x`) and applies the layers defined in the constructor in a specific sequence. First, the input is passed through the embedding layer to create embeddings of the input tokens. These embeddings are then permuted to be of the correct shape for the RNN layer, which expects inputs of the form (`seq_len, batch_size, H_in`). The RNN layer is then applied to these embeddings, producing both the RNN output (`rnn_out`) and the last hidden state (`self.last_hidden`). Finally, the output of the RNN is passed through a linear layer and flattened to produce the final output of the network, which is a sigmoid activation function applied to a tensor of shape (`batch_size`). This output is then returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nTY5n-t7X3VD"
      },
      "outputs": [],
      "source": [
        "class My_RNN(nn.Module):\n",
        "  def __init__(self, vocab_length, emb_dim, hidden_size, num_layers,\n",
        "               batch_size, RNN_type, bidirectional, device=device):\n",
        "    super().__init__()\n",
        "    self.emb_in_dim = vocab_length  # 10000\n",
        "    self.emb_out_dim = emb_dim  # 10\n",
        "    self.hidden_size = hidden_size \n",
        "    self.num_layers = num_layers\n",
        "    self.bidirectional = bidirectional\n",
        "    self.batch_size = batch_size\n",
        "    self.RNN_type = RNN_type\n",
        "    self.target_size = 1  # binary classification\n",
        "    self.device = device\n",
        "    self.D = 2 if self.bidirectional else 1  # enable/disable bidirectional\n",
        "    \n",
        "    valid_types = {'Simple RNN':RNN, 'LSTM':LSTM, 'GRU':GRU}\n",
        "    assert self.RNN_type in valid_types.keys(), f'You must choose one of {valid_types.keys()} types'\n",
        "    \n",
        "    self.embedding = Embedding(self.emb_in_dim, self.emb_out_dim) \n",
        "    \n",
        "    chosen_rnn = valid_types[self.RNN_type]\n",
        "    self.rnn = chosen_rnn(input_size=self.emb_out_dim, hidden_size=self.hidden_size,\n",
        "                          num_layers=self.num_layers, bidirectional = self.bidirectional)\n",
        "    \n",
        "    self.linear = Linear(self.D*self.hidden_size, self.target_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    #x: (batch_size, 500)\n",
        "    self.embeds = self.embedding(x)\n",
        "    #embeds: (batch_size, 500, 10)\n",
        "    #But rnn receives inputs of: (500, batch_size, 10) (seq_len, batch_size, H_in)\n",
        "    self.embeds = self.embeds.permute(1, 0, 2)\n",
        "    #embeds: (500, batch_size, 10) -> Now the shape is correct\n",
        "    rnn_out, self.last_hidden = self.rnn(self.embeds)\n",
        "    #rnn_out: (500, 256, 2*5:10) -> (seq_len, batch_size, D*hidden_size)\n",
        "    #h_n: (1, 256, hidden_size) -> (D*num_layers, batch_size, hidden_size)\n",
        "    output = self.linear(rnn_out[-1, :, :])\n",
        "    #output: (256, 1) -> (batch_size, target_size)\n",
        "    output = output.flatten()\n",
        "    #output: (256) -> (batch_size)\n",
        "    return F.sigmoid(output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vgp7U3QNBsG"
      },
      "source": [
        "### [TO COMPLETE] Simple RNN\n",
        "\n",
        "We define a RNN model and evaluate its performance. Your task is to choose the right loss function and motivate your choice. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%STARTEXT`"
      ],
      "metadata": {
        "id": "ThbMAKIYNaIG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER: **TO COMPLETE**"
      ],
      "metadata": {
        "id": "XGsx7sh1NkQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%ENDTEXT`"
      ],
      "metadata": {
        "id": "rqkEBYKINp8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%STARTCODE`"
      ],
      "metadata": {
        "id": "aenXPyLPormh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCxnJfEAi6aX"
      },
      "outputs": [],
      "source": [
        "vocab_length = len(vocab)\n",
        "emb_dim = 10\n",
        "hidden_size = 32\n",
        "num_layers = 1\n",
        "RNN_type = 'Simple RNN' #possible choices -> ['Simple RNN', 'LSTM', 'GRU']\n",
        "bidirectional = False\n",
        "\n",
        "EPOCHS = 10\n",
        "lr = 1e-3\n",
        "\n",
        "model = My_RNN(vocab_length=vocab_length, emb_dim=emb_dim, hidden_size=hidden_size,\n",
        "               num_layers=num_layers, batch_size=batch_size, RNN_type=RNN_type,\n",
        "               bidirectional=bidirectional).to(device)\n",
        "\n",
        "criterion = #[TO COMPLETE]     \n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%ENDCODE`\n",
        "\n",
        "Now we can train our model. Try to grasp how a training loop is defined in PyTorch and what each line does!"
      ],
      "metadata": {
        "id": "fEeUs41UowgR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPy8m4hPUGbs"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, dataloader_train, dataloader_val, epochs=EPOCHS):\n",
        "  loss_train, loss_val = [], []\n",
        "  acc_train, acc_val = [], []\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_acc_train, total_count_train, n_train_batches, total_loss_train = 0, 0, 0, 0\n",
        "    for idx, (label, text) in enumerate(dataloader_train):\n",
        "      optimizer.zero_grad()\n",
        "      logits = model(text)\n",
        "      loss = criterion(logits, label)\n",
        "      total_loss_train += loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      labels_form_logits = lambda x: 0. if x < 0.5 else 1.\n",
        "      logits = torch.tensor(list(map(labels_form_logits, logits))).to(model.device)\n",
        "      total_acc_train += (logits == label).sum().item()\n",
        "      total_count_train += label.size(0)\n",
        "      n_train_batches += 1\n",
        "\n",
        "    avg_loss_train = total_loss_train/n_train_batches\n",
        "    loss_train.append(avg_loss_train.item())\n",
        "    accuracy_train = total_acc_train/total_count_train\n",
        "    acc_train.append(accuracy_train)\n",
        "    \n",
        "    total_acc_val, total_count_val, n_val_batches, total_loss_val = 0, 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for idx, (label, text) in enumerate(dataloader_val):\n",
        "            logits = model(text)\n",
        "            loss = criterion(logits, label)\n",
        "            total_loss_val += loss\n",
        "            logits = torch.tensor(list(map(labels_form_logits, logits))).to(model.device)\n",
        "            total_acc_val += (logits == label).sum().item()\n",
        "            total_count_val += label.size(0)\n",
        "            n_val_batches += 1\n",
        "    avg_loss_val = total_loss_val/n_val_batches\n",
        "    loss_val.append(avg_loss_val.item())\n",
        "    accuracy_val = total_acc_val/total_count_val\n",
        "    acc_val.append(accuracy_val) \n",
        "    if epoch % 1 == 0:\n",
        "      print(f\"epoch: {epoch+1} -> Accuracy: {100*accuracy_train:.2f}%, Loss: {avg_loss_train:.8f}\",end=\" ---------------- \")\n",
        "      print(f\"Val_Acc: {100*accuracy_val:.2f}%, Val_Loss: {avg_loss_val:.8f}\")\n",
        "  return loss_train, acc_train, loss_val, acc_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSiNr8G0UQUK"
      },
      "outputs": [],
      "source": [
        "start = timer()\n",
        "loss_train, accuracy_train, loss_val, accuracy_val = train(model, optimizer, dataloader_training, dataloader_validation, epochs=EPOCHS)\n",
        "end = timer()\n",
        "print(f\"Training time in second: {(end - start)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's time to plot the results..."
      ],
      "metadata": {
        "id": "Ju0vOeHm4YNQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uX9Zb5OPVuFV"
      },
      "outputs": [],
      "source": [
        "def plot_learning_acc_and_loss(loss_tr, acc_tr, loss_val, acc_val):\n",
        "\n",
        "    plt.figure(figsize=(8, 10))\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.grid()\n",
        "    plt.plot(range(EPOCHS), acc_tr, label='acc_training')\n",
        "    plt.plot(range(EPOCHS), acc_val, label='acc_validation')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.grid()\n",
        "    plt.plot(range(EPOCHS), loss_tr, label='loss_training')\n",
        "    plt.plot(range(EPOCHS), loss_val, label='loss_validation')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='best')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "plot_learning_acc_and_loss(loss_train, accuracy_train, loss_val, accuracy_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "...and see the performances on the test set:"
      ],
      "metadata": {
        "id": "Qdl6Dxr-4eH7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vaVsXbkXjQ1"
      },
      "outputs": [],
      "source": [
        "def test(model, dataloader_test=dataloader_test):\n",
        "  model.eval()\n",
        "  total_acc_test, total_count_test, n_batches_test, loss = 0, 0, 0, 0\n",
        "  for idx, (label, text) in enumerate(dataloader_test):\n",
        "      pre_label = model(text)\n",
        "      loss += criterion(pre_label, label)\n",
        "      labels_form_pre_label = lambda x: 0. if x < 0.5 else 1.\n",
        "      pre_label = torch.tensor(list(map(labels_form_pre_label, pre_label))).to(model.device)\n",
        "      total_acc_test += (pre_label == label).sum().item()\n",
        "      total_count_test += label.size(0)\n",
        "      n_batches_test += 1\n",
        "  accuracy_test = total_acc_test/total_count_test\n",
        "  loss_test = loss/n_batches_test\n",
        "  print(f\"Test Loss: {loss_test:.8f}\", end=' ---------- ')\n",
        "  print(f\"Test Accuracy: {100*accuracy_test:.4f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C3jbfYYWXjeM"
      },
      "outputs": [],
      "source": [
        "test(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdVuxhxzkrE6"
      },
      "source": [
        "We should achieve ~60% accuracy on test set. Lets see if we can improve further!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-qDVP6kLhvd"
      },
      "source": [
        "### [TO COMPLETE] LSTM\n",
        "\n",
        "In this Exercise, you have to implement [LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html) model, similar to the previous one that, instead of exploiting the RNN layer, use an LSTM layer. Print the model summary. Then, train it and plot the values of accuracy and loss. Finally, discuss the differences in terms of performance, the number of parameters, and training time. Note that you can use a different number of units than the one used in the RNN example."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%STARTCODE`"
      ],
      "metadata": {
        "id": "hYpb8ddP6ESy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iHvTKfDkLiti"
      },
      "outputs": [],
      "source": [
        "# [TO COMPLETE]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%ENDCODE`"
      ],
      "metadata": {
        "id": "Nv3ny_Sq6FIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%STARTEXT`"
      ],
      "metadata": {
        "id": "YlEv5O4E4rji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER: **TO COMPLETE** Discussion of the results"
      ],
      "metadata": {
        "id": "ktbhjmgG4rji"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%ENDTEXT`"
      ],
      "metadata": {
        "id": "1BlUYTfO4rji"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqL4xEQ4Li6v"
      },
      "source": [
        "### [TO COMPLETE] GRU\n",
        "\n",
        "In this Exercise, you have to implement [GRU](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html) model, similar to the previous one that, instead of exploiting the RNN layer, use an GRU layer. Print the model summary. Then, train it and plot the values of accuracy and loss. Finally, discuss the differences in terms of performance, the number of parameters, and training time. Note that you can use a different number of units than the one used in the RNN example."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%STARTCODE`"
      ],
      "metadata": {
        "id": "HjSYXw975t5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# [TO COMPLETE]"
      ],
      "metadata": {
        "id": "lnKN_DAp5nLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%ENDCODE`"
      ],
      "metadata": {
        "id": "VgkezO3k56YF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%STARTEXT`"
      ],
      "metadata": {
        "id": "emB17gN7405X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER: **TO COMPLETE** Discussion of the results"
      ],
      "metadata": {
        "id": "WpWLN4iS405e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%ENDTEXT`"
      ],
      "metadata": {
        "id": "CmkBJ2If405f"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtrs1A7zSGwz"
      },
      "source": [
        "#BiDirectional\n",
        "\n",
        "Let's also have a look at the performances of a bidirectional LSTM instead of a simple LSTM. Try to understand what is the difference with respect to the previous model and use again the same loss you used before. This is just an extra model, so it will not be graded.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izJi1TbrSIbD"
      },
      "outputs": [],
      "source": [
        "vocab_length = len(vocab)\n",
        "emb_dim = 10\n",
        "hidden_size = 32\n",
        "num_layers = 1\n",
        "RNN_type = 'LSTM' #possible choices -> ['Simple RNN', 'LSTM', 'GRU']\n",
        "bidirectional = True\n",
        "\n",
        "EPOCHS = 15\n",
        "lr = 3e-4\n",
        "\n",
        "model = My_RNN(vocab_length=vocab_length, emb_dim=emb_dim, hidden_size=hidden_size,\n",
        "               num_layers=num_layers, batch_size=batch_size, RNN_type=RNN_type,\n",
        "               bidirectional=bidirectional).to(device)\n",
        "\n",
        "criterion = # COMPLETE\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "summary(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TMxq5jBS12G"
      },
      "outputs": [],
      "source": [
        "start = timer()\n",
        "loss_train, accuracy_train, loss_val, accuracy_val = train(model, optimizer, dataloader_training, dataloader_validation, epochs=EPOCHS)\n",
        "end = timer()\n",
        "print(f\"Training time in second: {(end - start)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsUza8zGS76N"
      },
      "outputs": [],
      "source": [
        "plot_learning_acc_and_loss(loss_train, accuracy_train, loss_val, accuracy_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9JTPXshS784"
      },
      "outputs": [],
      "source": [
        "test(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5kk32TU_ajUS"
      },
      "source": [
        "# Explore relations between words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjv9g8A0Fs3X"
      },
      "source": [
        "We will now quickly explore the properties of the embeddings learned by the model. Each embedding encodes the meaning of a word inferring it from the way it is used in the dataset. One possible way to explore the meaning encoded in the embeddings is check whether analogies that we make between concepts are reflected also in the embeddings as geometric properties. In particular, we will compute the difference between the embeddings of two related words, thus encoding their relation in a vector. Then, we will compute the same measure between a few couples of vectors and check if the couple which has the most similar measure corresponds to words that have the same kind of relation as the first ones. You can try different couples of words, and you may find out that sometimes the encoding of words meanings is the one we might expect - that's why carefully choosing the training data is very important!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5ljHjPJak_D"
      },
      "outputs": [],
      "source": [
        "def word_embedder(word):\n",
        "  word_high_dim = sent_padding(text_pipeline(word), maxlen=seq_len).to(device)\n",
        "  word_low_dim_embedded = model.embedding(word_high_dim)[-1]\n",
        "  return word_low_dim_embedded\n",
        "\n",
        "EMB_VOCAB = {}\n",
        "for word in vocab.get_itos():\n",
        "  EMB_VOCAB[word] =  word_embedder(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t64LwUq7iO6F"
      },
      "outputs": [],
      "source": [
        "def cosine_similarity(u, v):\n",
        "    return torch.dot(u,v) / (torch.sqrt(torch.sum(u * u)) * torch.sqrt(torch.sum(v * v)))\n",
        "\n",
        "def word_matcher(ref1, ref2, tgt3, candidates, EMB_VOCAB):\n",
        "  max_thr = -np.inf\n",
        "  e_ref1, e_ref2, e_tgt3 = EMB_VOCAB[ref1], EMB_VOCAB[ref2], EMB_VOCAB[tgt3]\n",
        "  for w in candidates:\n",
        "    e_w = EMB_VOCAB[w]\n",
        "    sim = cosine_similarity(e_ref2 - e_ref1, e_w - e_tgt3)\n",
        "    if sim > max_thr:\n",
        "      result = w\n",
        "      max_thr = sim\n",
        "  return result\n",
        "\n",
        "w1, w2, w3 = 'man', 'woman', 'king'\n",
        "w4_cand = ['soldier', 'queen', 'prophet']\n",
        "w4 = word_matcher(w1, w2, w3, w4_cand, EMB_VOCAB)\n",
        "print(f'The relation between {w1} -> {w2} is like the realtion between {w3} -> {w4}')\n",
        "\n",
        "w1, w2, w3 = 'germany', 'berlin', 'italy'\n",
        "w4_cand = ['spain', 'rome', 'germany', 'france']\n",
        "w4 = word_matcher(w1, w2, w3, w4_cand, EMB_VOCAB)\n",
        "print(f'The relation between {w1} -> {w2} is like the realtion between {w3} -> {w4}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBzxjzuvaTiC"
      },
      "source": [
        "### [TO COMPLETE] Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-IuFF0KGL0E"
      },
      "source": [
        "Let's now use a [Transformer](https://arxiv.org/abs/1706.03762) to perform the same task considered in the previous exercise. \n",
        "\n",
        "The structure of the transformer is defined as follows:\n",
        "*   A multi-head attention layer\n",
        "*   Dropout operation (`dropout_att`)\n",
        "*   Layer Normalization (`layernorm_att`)\n",
        "*   A feedforward Neural Network, Sequential, and Dense layer\n",
        "*   Dropout operation (`dropout_fnn`)\n",
        "*   Layer Normalization (`layernorm_fnn`) that has in input the summation of the attention layer output and the feedforward NN output\n",
        "\n",
        "Your task is to experiment with different hyperparameters values and try to find a configuration of the Transformer that can beat the RNNs. In the cell below, write a short comment on the impact of each hyperparameter on model performance according to your observations."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%STARTEXT`"
      ],
      "metadata": {
        "id": "UiAXqN5Q6NUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANSWER: **[TO COMPLETE]**"
      ],
      "metadata": {
        "id": "v3PisUPh6Njm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%ENDTEXT`"
      ],
      "metadata": {
        "id": "pYuC-_JV6NyW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvDpePVEbqkF"
      },
      "outputs": [],
      "source": [
        "new_seq_len = 200\n",
        "train_dataset_trns = CustomDataset(train_data, seq_len=new_seq_len)\n",
        "val_dataset_trns = CustomDataset(val_data, seq_len=new_seq_len)\n",
        "test_dataset_trns = CustomDataset(test_data, seq_len=new_seq_len)\n",
        "\n",
        "batch_size=256\n",
        "dataloader_training_trns = DataLoader(train_dataset_trns, batch_size=batch_size, shuffle=True)\n",
        "dataloader_validation_trns = DataLoader(val_dataset_trns, batch_size=batch_size)\n",
        "dataloader_test_trns = DataLoader(test_dataset_trns, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHI6PHorfKmo"
      },
      "outputs": [],
      "source": [
        "class My_Transform(nn.Module):\n",
        "  def __init__(self, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, dropout, layer_norm_eps, device=device):\n",
        "    super().__init__()\n",
        "    self.device = device\n",
        "\n",
        "    self.emb_en = Embedding(num_embeddings=len(vocab), embedding_dim=d_model)\n",
        "    self.emb_de = Embedding(num_embeddings=2, embedding_dim=d_model)\n",
        "    #2: because of binary classification\n",
        "\n",
        "    self.transformer = Transformer(d_model=d_model, nhead=nhead, num_encoder_layers=num_encoder_layers,\n",
        "                                   num_decoder_layers=num_decoder_layers, dim_feedforward=dim_feedforward,\n",
        "                                   dropout=dropout, layer_norm_eps=layer_norm_eps)\n",
        "    \n",
        "    self.linear = Linear(d_model, 1)#1: because of binary classification\n",
        "\n",
        "  def forward(self, x, y):\n",
        "\n",
        "    #x: (batch_size, seq_len) -> (256, 200)\n",
        "    #y: (batch_size) -> (256)\n",
        "\n",
        "    y = torch.roll(y, shifts=1, dims=0) #right shifted\n",
        "\n",
        "    self.embedded_src = self.emb_en(x)\n",
        "    #self.embedded_src: (batch_size, seq_len, d_model) -> (256, 200, 5)\n",
        "\n",
        "    '''\n",
        "    Transformer requires src_dim and trg_dim of (S, N, E)\n",
        "    (S, N, E) -> (Seq_len, Batch_size, Embed_dim) -> (seq_len, batch_size, d_model)\n",
        "        - self.embedded_src must be permuted\n",
        "        - self.embedded_trg must be reshaped accordingly\n",
        "    Transformer will produce an output of dim (T, N, E)\n",
        "    (T, N, E) -> (Target_len, Batch_size, Embed_dim) -> (1, batch_size, d_model)\n",
        "    '''\n",
        "    \n",
        "    self.embedded_src = self.embedded_src.permute(1, 0, 2)\n",
        "    #self.embedded_src: (seq_len, batch_size, d_model) -> (200, 256, 5)\n",
        "    self.embedded_trg = self.emb_de(y.unsqueeze(0).long()) \n",
        "    #self.embedded_trg: (seq_len:target_len, batch_size, d_model) -> (1, 256, 5)\n",
        "\n",
        "    trns_out = self.transformer(self.embedded_src, self.embedded_trg)\n",
        "    #trns_out: (target_len, batch_size, d_model) -> (1, 256, 5)\n",
        "\n",
        "    out = self.linear(trns_out)\n",
        "    #out: (1, 256, 1)\n",
        "\n",
        "    return F.sigmoid(out.squeeze()) #(batch_size) -> (256) : Like y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%STARTCODE`"
      ],
      "metadata": {
        "id": "TIQ5fVfpBc8X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eg7oohRFUUqm"
      },
      "outputs": [],
      "source": [
        "d_model= #[TO COMPLETE]\n",
        "nhead= #[TO COMPLETE]                  \n",
        "num_encoder_layers= #[TO COMPLETE]     \n",
        "num_decoder_layers= #[TO COMPLETE]     \n",
        "dim_feedforward=128\n",
        "dropout=0.1\n",
        "layer_norm_eps=1e-5\n",
        "\n",
        "EPOCHS = 10\n",
        "lr = 1e-3\n",
        "\n",
        "transformer = My_Transform(d_model, nhead, num_encoder_layers, num_decoder_layers,\n",
        "                           dim_feedforward, dropout, layer_norm_eps).to(device)\n",
        "\n",
        "criterion = torch.nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=lr)\n",
        "\n",
        "summary(transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%ENDCODE`"
      ],
      "metadata": {
        "id": "HSXA45bUBmmO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-8Bii1lbBNO"
      },
      "outputs": [],
      "source": [
        "def train_trns(model, optimizer, dataloader_train, dataloader_val, epochs=EPOCHS):\n",
        "  loss_train, loss_val = [], []\n",
        "  acc_train, acc_val = [], []\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_acc_train, total_count_train, n_train_batches, total_loss_train = 0, 0, 0, 0\n",
        "    for idx, (label, text) in enumerate(dataloader_train):\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      logits = model(text, label)\n",
        "      loss = criterion(logits, label)\n",
        "      total_loss_train += loss\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      labels_form_logits = lambda x: 0. if x < 0.5 else 1.\n",
        "      logits = torch.tensor(list(map(labels_form_logits, logits))).to(model.device)\n",
        "      total_acc_train += (logits == label).sum().item()\n",
        "      total_count_train += label.size(0)\n",
        "      n_train_batches += 1\n",
        "\n",
        "    avg_loss_train = total_loss_train/n_train_batches\n",
        "    loss_train.append(avg_loss_train.item())\n",
        "    accuracy_train = total_acc_train/total_count_train\n",
        "    acc_train.append(accuracy_train)\n",
        "    \n",
        "    total_acc_val, total_count_val, n_val_batches, total_loss_val = 0, 0, 0, 0\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for idx, (label, text) in enumerate(dataloader_val):\n",
        "\n",
        "            logits = model(text, label)\n",
        "            loss = criterion(logits, label)\n",
        "            total_loss_val += loss\n",
        "            logits = torch.tensor(list(map(labels_form_logits, logits))).to(model.device)\n",
        "            total_acc_val += (logits == label).sum().item()\n",
        "            total_count_val += label.size(0)\n",
        "            n_val_batches += 1\n",
        "    avg_loss_val = total_loss_val/n_val_batches\n",
        "    loss_val.append(avg_loss_val.item())\n",
        "    accuracy_val = total_acc_val/total_count_val\n",
        "    acc_val.append(accuracy_val) \n",
        "    if epoch % 1 == 0:\n",
        "      print(f\"epoch: {epoch+1} -> Accuracy: {100*accuracy_train:.2f}%, Loss: {avg_loss_train:.8f}\",end=\" ---------------- \")\n",
        "      print(f\"Val_Acc: {100*accuracy_val:.2f}%, Val_Loss: {avg_loss_val:.8f}\")\n",
        "  return loss_train, acc_train, loss_val, acc_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7oNBz4SaAQQ"
      },
      "outputs": [],
      "source": [
        "start = timer()\n",
        "loss_train, accuracy_train, loss_val, accuracy_val = train_trns(transformer, optimizer, dataloader_training_trns, dataloader_validation_trns, epochs=EPOCHS)\n",
        "end = timer()\n",
        "print(f\"Training time in second: {(end - start)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%STARTCODE`"
      ],
      "metadata": {
        "id": "0tNA0lsOGDf3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_4Z_R15dc5L"
      },
      "outputs": [],
      "source": [
        "plot_learning_acc_and_loss(loss_train, accuracy_train, loss_val, accuracy_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%ENDCODE`"
      ],
      "metadata": {
        "id": "olEFVN6FGEvA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3FH0yRvb2o3"
      },
      "outputs": [],
      "source": [
        "def test_trns(model, dataloader_test):\n",
        "  model.eval()\n",
        "  total_acc_test, total_count_test, n_batches_test, loss = 0, 0, 0, 0\n",
        "  for idx, (label, text) in enumerate(dataloader_test):\n",
        "      pre_label = model(text, label)\n",
        "      loss += criterion(pre_label, label)\n",
        "      labels_form_pre_label = lambda x: 0. if x < 0.5 else 1.\n",
        "      pre_label = torch.tensor(list(map(labels_form_pre_label, pre_label))).to(model.device)\n",
        "      total_acc_test += (pre_label == label).sum().item()\n",
        "      total_count_test += label.size(0)\n",
        "      n_batches_test += 1\n",
        "  accuracy_test = total_acc_test/total_count_test\n",
        "  loss_test = loss/n_batches_test\n",
        "  print(f\"Test Loss: {loss_test:.8f}\", end=' ---------- ')\n",
        "  print(f\"Test Accuracy: {100*accuracy_test:.4f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%STARTCODE`"
      ],
      "metadata": {
        "id": "qLuAUBO7F3Mb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrNFjrQKc0iY"
      },
      "outputs": [],
      "source": [
        "gc.collect()\n",
        "test_trns(transformer, dataloader_test_trns)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`%ENDCODE`"
      ],
      "metadata": {
        "id": "OZTFCf74F8_f"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}