{"cells":[{"cell_type":"markdown","source":["#**Deep Learning Homework 2: *Optimize and Train Deep Models***\n","### MSc Computer Science, Data Science, Cybersecurity @UNIPD\n","### 2nd semester - 6 ECTS\n","### Prof. Alessandro Sperduti, Prof. NicolÃ² Navarin and Dr. Luca Pasa\n","---\n","In this homework, we will explore how to develop a simple Deep Neural Network for a classification problem. We will learn how to use one of the most popular libraries for Deep Learning, namely [PyTorch ðŸ”¥](https://pytorch.org/). Then, we will see how to face a well known problem that is common during the training phase---overfitting on the training set. Finally, we will study how to perform a fair model selection. \n","\n","<u>Disclaimer</u>: PyTorch is a complex framework actively used by researcher and developers all over the word. It is highly customizable and there are many other packages build along it. The pourpouse of this and the next homeworks is not to go into all the details of its functionalities, but rather to give you an overview of its applications and understand its rationale. There are many good tutorials online and we suggest you to look at the official [Learn the Basics Tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html), in particular if you are not familiar with other Deep Learning Frameworks. \n","\n","We hope that you will be able to use PyTorch by yourself for your future projects! \n","---\n","##**Important Instructions for Submissions:**\n","\n","Generally, in the homeworks, you will be either required to complete a part of Python code or to answer questions in text cells. Code and text cells where you are expected to write your answers have been marked by `%STARTCODE` and `%ENDCODE` or `%STARTEXT` and `%ENDTEXT` tags, respectively. Note that you should never change, move or remove these two tags, otherwise your answers will be __not__ valid. As you will see in this notebook, each cell that includes a `[TO COMPLETE]` part has been put between these placeholders. "],"metadata":{"id":"0aF3bkrZNSA9"}},{"cell_type":"markdown","metadata":{"id":"IqHD-0bAm6y7"},"source":["## Requirements"]},{"cell_type":"markdown","source":["In this first exercise we will develop a deep feed forward neural network to perform text classification.\n","\n","Let's start importing the libraries we will need and setting a couple of environmental variables."],"metadata":{"id":"G81vdttiN4Gr"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3T-4nPKnbj6E"},"outputs":[],"source":["# They've just released torch 2.0, but we will use the more stable torch=1.13 along with other supporting libriaries\n","!pip3 install datasets skorch pandas~=1.5 torch~=1.13 torchinfo torchdata~=0.5 torchtext~=0.14 torchvision~=0.14 torchaudio~=0.13"]},{"cell_type":"markdown","metadata":{"id":"dz4MTTs-m3qG"},"source":["##Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j1YNozG9CRwL"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","from torchinfo import summary\n","from torchtext.datasets import AG_NEWS\n","from torchtext.data.utils import get_tokenizer\n","from torchtext.vocab import build_vocab_from_iterator\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Dataset\n","from torch import nn\n","from sklearn.model_selection import GridSearchCV\n","from skorch import NeuralNetClassifier \n","from skorch.helper import SliceDataset\n","import multiprocessing\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import logging\n","import os\n","from timeit import default_timer as timer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BNgvj4naZajF"},"outputs":[],"source":["logging.disable(logging.WARNING)\n","os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" \n","\n","print(f\"{torch.__version__=}\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"{device=}\")\n","!nvidia-smi --format=csv --query-gpu=index,name,driver_version,memory.total,memory.used,memory.free\n","\n","# Set seed for reproducibility\n","torch.manual_seed(42)\n","rng = np.random.default_rng(seed=4242)"]},{"cell_type":"markdown","metadata":{"id":"rt1fhAbVmpTx"},"source":["#Data Loading and Preprocessing"]},{"cell_type":"markdown","source":["### Load Data: AG News Subset\n","\n","In this HW, we use the AG News Subset that is available in Torchtext Datasets. The AG's news topic classification dataset is constructed by choosing the 4 largest topic classes (1-World, 2-Sports, 3-Business, 4-Sci/Tech) from a larger news corpus. The total number of training samples is $120.000$ and testing $7.600$. For resource constraints, we will limit the number of training, validation and test samples to $10.000$, $1.000$ and $1.000$, respectively. Each sample contains both the title and an excerpt of the article, but in this exercise we will use only the excerpt as input."],"metadata":{"id":"WqQZIK0WO908"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"b1nwREGYCfSb"},"outputs":[],"source":["train, test = AG_NEWS(root=\"dataset\", split=('train', 'test'))\n","Tr, Ts = list(train), list(test)\n","rng.shuffle(Tr)\n","rng.shuffle(Ts)\n","len_train, len_val, len_test = 10000, 1000, 1000\n","train_data, val_data, test_data = Tr[:len_train], Ts[:len_val], Ts[len_val:len_val+len_test]"]},{"cell_type":"markdown","source":["And have a look at one sample:"],"metadata":{"id":"oS1B3TjjtoAe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RdoLOjK2CfVR"},"outputs":[],"source":["idx = 10\n","sample_label, sample_text = train_data[idx]\n","print(f\"Text: {sample_text}\")\n","print(f\"Label: {sample_label}\")"]},{"cell_type":"markdown","source":["### Data Preprocessing\n","We will go though the basic data processing building blocks for raw text string.\n","Given a string, the first step is to tokenize the data to prepare it as input for the model. For this purpouse, we will use the basic Tokenizer of PyTorch. As you can see, it just normalizes the text, removes whitespaces and unsupported characters, and outputs a list:"],"metadata":{"id":"SpTSr7pruC0u"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ktz69_M1DITk"},"outputs":[],"source":["tokenizer = get_tokenizer('basic_english')\n","print(\"Example: \", tokenizer(\"I am so HAPPY to study Deep Learning! #UNIPD #UniLIFE\"))"]},{"cell_type":"markdown","source":["The AG News Subset dataset consists in news headlines---sequences of words. We have to encode each sample in a single tensor with a fixed number of elements, we will build a vocabulary of the headlines in our training set. In other words, we tokenize all the training set, and map each unique word into a number (a.k.a. index). This is necesseray as Neural Networks process numbers, so such vocabulary simply translates between readable words and numbers."],"metadata":{"id":"qpIcUP8AuKMg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hw0KabxGEGqj"},"outputs":[],"source":["def create_tokens(dataset):\n","  for sample in dataset:\n","    yield tokenizer(sample[1])\n","\n","# We limit our model to learn from the first 1000 most frequent tokens\n","vocab = build_vocab_from_iterator(create_tokens(train_data), specials=[\"<unk>\"], max_tokens=1000) # <unk> is the index we use for specials characters ...\n","vocab.set_default_index(vocab[\"<unk>\"]) # ... and as 0 the default values for tokens out of the vocabulary (OOV)\n","print(f\"Our vocabulary is made of {len(vocab)} tokens-index pairs.\")"]},{"cell_type":"markdown","source":["This vocabulary was build from the training set. Let us see what it does on our example:"],"metadata":{"id":"q6FeI-HavMKA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"b4ZoAGVPCfc0"},"outputs":[],"source":["# Let us define some useful functions to handle tokens and labels\n","text_pipeline = lambda x: vocab(tokenizer(x)) # function to go from string -> tokens\n","label_pipeline = lambda x: int(x) - 1 # function to label starting form 0 for indexing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ApAbzk_3Cffc"},"outputs":[],"source":["sample_tokenization = text_pipeline(sample_text)\n","sample_label_idx = label_pipeline(sample_label)\n","print(f\"Samlpe headline text:\\n {sample_text}\")\n","print(f\"Sample headline to tokens:\\n {sample_tokenization}\")\n","print(f\"Label:\\n {sample_label}\")\n","print(f\"Label index:\\n {sample_label_idx}\")"]},{"cell_type":"markdown","source":["So each word is correctly mapped into a number, where most frequent words come first (see that the preposition \"of\", which is very frequent, gets translated to the number $6$). Notice also how \"Vancouver\" takes the default index value of $0$, meaning that is not frequent in the training set. \n","\n","Neural Netowrks (usually) take input of fixed size, but news aricles have different lenghts! To tackle this issue, one way is to resort to _multi-hot-encoding_ our dataset, means turning it into a tensor of $0$ and $1$. Concretely, this would mean for instance turning the sequence `[3, 5]` into a $1000$-dimensional vector (our vocabulary) that would be all zeros except for indices 3 and 5, which would be ones. The obtained input representation indicates which words are present (at least one time) in the sentence."],"metadata":{"id":"g030w_uaiKoM"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"aQgZmtZiFE7X"},"outputs":[],"source":["def multi_hot(token_list, n_cat=len(vocab)):\n","  encoded = [1 if i in token_list else 0 for i in range(n_cat)]\n","  return encoded"]},{"cell_type":"markdown","source":["The next step is building a PyTorch `DataLoader`. Without going too much into the details, a `DataLoader` is the object one uses in the `for` loops when iterating over the data for training or inference. It has some additional benefits and pourpouses over iterating usual lists, such as multiprocessing, data transformation, sampling or shuffling. Don't worry too much for now, as we do this part for you. "],"metadata":{"id":"RqWBKCchxby4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rJCAhS76FFDL"},"outputs":[],"source":["class AGNewsDataset(Dataset):\n","  def __init__(self, dataset):\n","    super().__init__()\n","    self.dataset = dataset\n","  def __len__(self):\n","    return len(self.dataset)\n","  def __getitem__(self, idx):\n","    label, text = self.dataset[idx]\n","    lb = label_pipeline(label)\n","    txt = multi_hot(text_pipeline(text))\n","    lb, txt = torch.tensor(lb, dtype=torch.uint8, device=device), torch.tensor(txt, dtype=torch.float32, device=device)\n","    return lb, txt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JF5JMnjfGuCx"},"outputs":[],"source":["train_dataset = AGNewsDataset(train_data)\n","val_dataset = AGNewsDataset(val_data)\n","test_dataset = AGNewsDataset(test_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tHUTf_g8Gy75"},"outputs":[],"source":["batch_size = 256\n","dataloader_training = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n","dataloader_validation = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"markdown","metadata":{"id":"wcn4Z74SmwaC"},"source":["# Exercise 2.1: Model Definition and Training\n","We can now define the 3-layers feed forward model!\n","In PyTorch we do that by subclassing `nn.Module`, and initialize the neural network layers in `__init__`. Every `nn.Module` subclass implements the operations on input data in the `forward` method.\n","\n","In detail, we created a `FeedforwardNetwork` class. The constructor takes three arguments: `input_dim` (an integer), which is the dimension of the input layer; `num_classes` (an integer), which is the number of output classes; and `hidden_layers_dim` (a list of integers), which is the dimension of the hidden layers in the network.\n","\n","In the constructor, the `nn.ModuleList() ` object is initialized to hold the layers of the network. If `hidden_layers_dim` is an empty list, then the network only consists of a single linear layer (from input to output). Otherwise, the network consists of multiple linear layers. The first layer goes from the input layer to the first hidden layer, and the subsequent hidden layers go from the previous hidden layer to the next hidden layer. The final output layer goes from the last hidden layer to the output layer.\n","\n","The `_init_weights` function is a helper function that initializes the weights of the linear layers in the network. It initializes the weights using a normal distribution with a mean of 0 and a standard deviation of 0.1, and sets the biases to zero. Notice that there exist other methods available in the `torch.nn.init` module to initialize layers' weights.\n","\n","The `forward` function defines the forward pass of the network. It first checks whether the network has only one layer (in which case it simply returns the output of that layer). Otherwise, it applies the `relu` activation function to the output of each hidden layer and passes the result to the next layer. Finally, it returns the output of the final layer. We don't apply the softmax function to the logits produced by the network as this will be done internally in the `CrossEntropyLoss` class, as documented [here](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)."]},{"cell_type":"code","source":["class FeedforwardNetwork(nn.Module):\n","  \n","  def __init__(self, input_dim, num_classes, hidden_layers_dim=[]):\n","    super().__init__()\n","    self.layers = nn.ModuleList()\n","    if len(hidden_layers_dim) == 0:\n","      self.layers = self.layers.append(nn.Linear(input_dim, num_classes))\n","    else:\n","      for layer_idx in range(len(hidden_layers_dim)):\n","        if layer_idx == 0:  # first layer, from input to hidden\n","          self.layers = self.layers.append(nn.Linear(input_dim, hidden_layers_dim[layer_idx]))\n","        else:  # hidden layers, depending on the input\n","          self.layers = self.layers.append(nn.Linear(hidden_layers_dim[layer_idx-1], hidden_layers_dim[layer_idx]))\n","      self.layers = self.layers.append(nn.Linear(hidden_layers_dim[-1], num_classes))  # final output layer\n","    self.apply(self._init_weights)\n","    \n","  def _init_weights(self, module):\n","    if isinstance(module, nn.Linear):\n","        module.weight.data.normal_(mean=0.0, std=.1)\n","        if module.bias is not None:\n","            module.bias.data.zero_()\n","  \n","  def forward(self, x):\n","    if len(self.layers) == 1:\n","      return self.layers[0](x)\n","    else:\n","      for layer in self.layers[:-1]:\n","        x = F.relu(layer(x))\n","    return self.layers[-1](x)"],"metadata":{"id":"PdtoVP6B_YmA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x77rElGcHJrn"},"outputs":[],"source":["# Now we prepare our model for this specific dataset and exercise\n","num_class = len(set([label for (label, text) in train_data]))\n","vocab_size = len(vocab)\n","EPOCHS = 40\n","lr = 1e-4\n","model = FeedforwardNetwork(vocab_size, num_class, hidden_layers_dim=[64, 64]).to(device)\n","summary(model, (batch_size, vocab_size)) # Here is a nice summary of our model!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2lexuSj2IFo3"},"outputs":[],"source":["# We need to defin our loss function (Cross Entropy for multiclass classification) and optimizer\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)"]},{"cell_type":"markdown","source":["Finally, we can define the training procedure. In PyTorch there are few guidelines to follow to train a model. This time try on your own to understand what each line of code is doing, it will be required for the following HWs and exercises!"],"metadata":{"id":"0uzGOnHS0uHu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMMREK-5I2xP"},"outputs":[],"source":["def train(model, optimizer, dataloader_train, dataloader_val, EPOCHS=EPOCHS):\n","  loss_train, loss_val = [], []\n","  acc_train, acc_val = [], []\n","  for epoch in range(EPOCHS):\n","\n","    model.train()\n","    total_acc_train, total_count_train, n_train_batches, total_loss_train = 0, 0, 0, 0\n","    for idx, (label, text) in enumerate(dataloader_train):\n","      optimizer.zero_grad()\n","      logits = model(text)\n","      loss = criterion(logits, label)\n","      total_loss_train += loss\n","      loss.backward()\n","      optimizer.step()\n","\n","      total_acc_train += (logits.argmax(1) == label).sum().item()\n","      total_count_train += label.size(0)\n","      n_train_batches += 1\n","\n","    avg_loss_train = total_loss_train/n_train_batches\n","    loss_train.append(avg_loss_train.item())\n","    accuracy_train = total_acc_train/total_count_train\n","    acc_train.append(accuracy_train)\n","    \n","    total_acc_val, total_count_val, n_val_batches, total_loss_val = 0, 0, 0, 0\n","    with torch.no_grad():\n","        model.eval()\n","        for idx, (label, text) in enumerate(dataloader_val):\n","            logits = model(text)\n","            loss = criterion(logits, label)\n","            total_loss_val += loss\n","            total_acc_val += (logits.argmax(1) == label).sum().item()\n","            total_count_val += label.size(0)\n","            n_val_batches += 1\n","    avg_loss_val = total_loss_val/n_val_batches\n","    loss_val.append(avg_loss_val.item())\n","    accuracy_val = total_acc_val/total_count_val\n","    acc_val.append(accuracy_val) \n","    if epoch % 1 == 0:\n","      print(f\"epoch: {epoch+1} -> Accuracy: {100*accuracy_train:.2f}%, Loss: {avg_loss_train:.8f}\",end=\" ---------------- \")\n","      print(f\"Val_Acc: {100*accuracy_val:.2f}%, Val_Loss: {avg_loss_val:.8f}\")\n","  return loss_train, acc_train, loss_val, acc_val"]},{"cell_type":"markdown","source":["Let's train it!"],"metadata":{"id":"QSWOoJxe3cp-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"fa54R_1MKbWb"},"outputs":[],"source":["start = timer()\n","loss_train, accuracy_train, loss_val, accuracy_val = train(model, optimizer, dataloader_training, dataloader_validation)\n","end = timer()\n","print(f\"Training time in second: {(end - start)}\")"]},{"cell_type":"markdown","source":["We can now plot these results:"],"metadata":{"id":"TvoclZ6D3pu8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6y0ZMP26Q_oP"},"outputs":[],"source":["def plot_learning_acc_and_loss(loss_tr, acc_tr, loss_val, acc_val):\n","    info = {'loss_training':loss_tr,'acc_trainig':acc_tr, 'loss_validation':loss_val, 'acc_validation':acc_val}\n","    df = pd.DataFrame(info)\n","    df.plot(figsize=(8, 10), subplots=[('acc_trainig', 'acc_validation'), ('loss_training','loss_validation')], grid=True)\n","    plt.xlabel(\"Epochs\")\n","    plt.show()\n","plot_learning_acc_and_loss(loss_train, accuracy_train, loss_val, accuracy_val)"]},{"cell_type":"markdown","source":["### [TO COMPLETE] Evaluate the model:\n","\n","Now, our model has been optimized on the training set, and as you can see the performance on the validation set in quite similar (so it does not overfit the training data). Let's now evaluate the performance of our model using the test set. By having a look at the previous code, compute the performances on the test set."],"metadata":{"id":"K2rNayCQ3tzl"}},{"cell_type":"markdown","source":["`%STARTCODE`"],"metadata":{"id":"73rQ8jooC1I7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"deXxro4Mf7R6"},"outputs":[],"source":["model.eval()\n","total_acc_test, total_count_test, n_batches_test, loss = 0, 0, 0, 0\n","\n","#[TO COMPLETE]\n","\n","print(f\"Test Loss: {loss_test:.8f}\", end=' ---------- ')\n","print(f\"Test Accuracy: {100*accuracy_test:.4f}%\")"]},{"cell_type":"markdown","source":["`%ENDCODE`"],"metadata":{"id":"OpNJQzQXC2HE"}},{"cell_type":"markdown","source":["Explain why it is important to use test and validation, and why it is important to evaluate the model on the test set instead of the validation set. Finally, explain what is the usefulness of the validation set. Insert the discussion in the next cell.\n"],"metadata":{"id":"ZnIr1X-CDIuH"}},{"cell_type":"markdown","source":["\n","`%STARTEXT`"],"metadata":{"id":"boNBDg70Rv81"}},{"cell_type":"markdown","source":["Answer: **[TO COMLPETE]**"],"metadata":{"id":"hv-xTkk_DSE2"}},{"cell_type":"markdown","source":["`%ENDTEXT`"],"metadata":{"id":"i_u7VgTxDVm9"}},{"cell_type":"markdown","metadata":{"id":"vfQaJFqRVNon"},"source":["# Exercise 2.2: Overfiting\n","\n","A common problem that occurs when you train a deep neural network is overfittig. Overfitting occurs when you achieve a good fit of your model on the training data, while it does not generalize well on new, unseen data. In other words, the model learned patterns specific to the training data, which are irrelevant in other data.\n","As we have seen in the previous exercise, our model does not much overfit the training data. In this exercise, we try to modify the training parameters in order to have a model that overfits.\n","Overfitting can have many causes and usually is a combination of some of them, for instance: too many parameters/ layers, too few training samples, wrong learning rate (usualy too high), etc..\n"]},{"cell_type":"markdown","source":["## [TO COMPLETE] Overfitting Model\n","In the next cell define a new model (similar to the previuos one) that overfits the training data; then plot the trend of the loss in training and validation set.\n","\n"],"metadata":{"id":"v_UTGw7AENfn"}},{"cell_type":"markdown","source":["`%STARTCODE`"],"metadata":{"id":"SXbecVyNPG1x"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DfheR-DcjQgc"},"outputs":[],"source":["EPOCHS = #[TO COMPLETE]\n","lr = #[TO COMPLETE]\n","overfit_model = #[TO COMPLETE]\n","optimizer = torch.optim.Adam(overfit_model.parameters(), lr=lr)\n","summary(overfit_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NNeky13gjesY"},"outputs":[],"source":["start = timer()\n","loss_train, accuracy_train, loss_val, accuracy_val = train(overfit_model, optimizer, dataloader_training, dataloader_validation)\n","end = timer()\n","print(f\"Training time in second: {(end - start)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c9WMZ-mljuwH"},"outputs":[],"source":["plot_learning_acc_and_loss(loss_train, accuracy_train, loss_val, accuracy_val)"]},{"cell_type":"markdown","source":["`%ENDCODE`"],"metadata":{"id":"AhWJoj7QEYdY"}},{"cell_type":"markdown","metadata":{"id":"kpIi4-OBVWmq"},"source":["# Exercise 2.3: $L^1$- and $L^2$-regularization\n","One possible way to solve the overitting issue is by using regularization methods. The two most common regularization methods in Deep Learning are the L1-norm regularization and the L2-norm regularization. Both These techniques are based on limiting the capacity of models, by adding a parameter norm penalty to the objective function $\\mathcal{J}$:\n","$$\n","\\hat{\\mathcal{J}}(\\theta,\\mathbf{X},\\mathbf{y}) = \\mathcal{J}(\\theta,\\mathbf{X},\\mathbf{y}) + \\lambda L^p(\\theta)\n","$$\n","where $\\lambda$ is a hyperparameter that weighs the relative contribution of the norm penalty $L^p$ on a lernable parameter vector $\\theta$:\n","$$\n"," L^p(\\theta)=||\\theta||_p=\\left(\\sum_i |\\theta_i|^p\\right)^{1/p}\n","$$"]},{"cell_type":"markdown","source":["In the following blocks you should implement both the $L^1$ and $L^2$ regularization for the overfitting model.\n","\n","**Disclaimer**: you can implement norms either from scratch (i.e. using simple math operators) or using `torch.linalg` methods, but **not** using implementations embedded in Torch optimizers. "],"metadata":{"id":"ySvmdaBwGqkE"}},{"cell_type":"markdown","source":["### [TO COMPLETE] $L^1$ Regularization"],"metadata":{"id":"PNWyA_jvFKLx"}},{"cell_type":"markdown","source":["`%STARTCODE`"],"metadata":{"id":"gWrao8fWO1X1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mfsi9YeuQyj9"},"outputs":[],"source":["def train_L1(model, optimizer, dataloader_train, dataloader_val, l1_lambda=[TO COMPLETE], EPOCHS=EPOCHS):\n","  loss_train, loss_val = [], []\n","  acc_train, acc_val = [], []\n","  for epoch in range(EPOCHS):\n","\n","    model.train()\n","    total_acc_train, total_count_train, n_train_batches, total_loss_train = 0, 0, 0, 0\n","    for idx, (label, text) in enumerate(dataloader_train):\n","      optimizer.zero_grad()\n","      logits = model(text)\n","      loss = criterion(logits, label)\n","      ##########################################################\n","      #[TO COMPLETE]\n","      loss_with_l1 = \n","      ##########################################################\n","      total_loss_train += loss\n","      loss_with_l1.backward()\n","      optimizer.step()\n","\n","      total_acc_train += (logits.argmax(1) == label).sum().item()\n","      total_count_train += label.size(0)\n","      n_train_batches += 1\n","\n","    avg_loss_train = total_loss_train/n_train_batches\n","    loss_train.append(avg_loss_train.item())\n","    accuracy_train = total_acc_train/total_count_train\n","    acc_train.append(accuracy_train)\n","    \n","    total_acc_val, total_count_val, n_val_batches, total_loss_val = 0, 0, 0, 0\n","    with torch.no_grad():\n","        model.eval()\n","        for idx, (label, text) in enumerate(dataloader_val):\n","            logits = model(text)\n","            loss = criterion(logits, label)\n","            total_loss_val += loss\n","            total_acc_val += (logits.argmax(1) == label).sum().item()\n","            total_count_val += label.size(0)\n","            n_val_batches += 1\n","    avg_loss_val = total_loss_val/n_val_batches\n","    loss_val.append(avg_loss_val.item())\n","    accuracy_val = total_acc_val/total_count_val\n","    acc_val.append(accuracy_val) \n","    if epoch % 1 == 0:\n","      print(f\"epoch: {epoch+1} -> Accuracy: {100*accuracy_train:.2f}%, Loss: {avg_loss_train:.8f}\",end=\" ---------------- \")\n","      print(f\"Val_Acc: {100*accuracy_val:.2f}%, Val_Loss: {avg_loss_val:.8f}\")\n","  return loss_train, acc_train, loss_val, acc_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f4vniuv9QH7Y"},"outputs":[],"source":["EPOCHS = 40\n","lr = 1e-4\n","overfit_model = FeedforwardNetwork(vocab_size, num_class, hidden_layers_dim=[256, 128, 64]).to(device)\n","optimizer = torch.optim.Adam(overfit_model.parameters(), lr=lr)\n","summary(overfit_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MBhivLkMSeQy"},"outputs":[],"source":["start = timer()\n","loss_train, accuracy_train, loss_val, accuracy_val = train_L1(overfit_model, optimizer, dataloader_training, dataloader_validation, l1_lambda=0.001)\n","end = timer()\n","print(f\"Training time in second: {(end - start)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q4UHBptprtk0"},"outputs":[],"source":["plot_learning_acc_and_loss(loss_train, accuracy_train, loss_val, accuracy_val)"]},{"cell_type":"markdown","source":["`%ENDCODE`"],"metadata":{"id":"fh5L4WPdGpRZ"}},{"cell_type":"markdown","source":["### [TO COMPLETE] $L^2$ Regularization\n"],"metadata":{"id":"T8c2h8JEGtHD"}},{"cell_type":"markdown","source":["\n","`%STARTCODE`"],"metadata":{"id":"XnFVbeGnRpTf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-pCqS8PjQymc"},"outputs":[],"source":["def train_L2(model, optimizer, dataloader_train, dataloader_val, l2_lambda=[TO COMPLETE], EPOCHS=EPOCHS):\n","  loss_train, loss_val = [], []\n","  acc_train, acc_val = [], []\n","  for epoch in range(EPOCHS):\n","\n","    model.train()\n","    total_acc_train, total_count_train, n_train_batches, total_loss_train = 0, 0, 0, 0\n","    for idx, (label, text) in enumerate(dataloader_train):\n","      optimizer.zero_grad()\n","      logits = model(text)\n","      loss = criterion(logits, label)\n","      ##########################################################\n","      #[TO COMPLETE]\n","      loss_with_l2 = \n","      ##########################################################\n","      total_loss_train += loss\n","      loss_with_l2.backward()\n","      optimizer.step()\n","\n","      total_acc_train += (logits.argmax(1) == label).sum().item()\n","      total_count_train += label.size(0)\n","      n_train_batches += 1\n","\n","    avg_loss_train = total_loss_train/n_train_batches\n","    loss_train.append(avg_loss_train.item())\n","    accuracy_train = total_acc_train/total_count_train\n","    acc_train.append(accuracy_train)\n","    \n","    total_acc_val, total_count_val, n_val_batches, total_loss_val = 0, 0, 0, 0\n","    with torch.no_grad():\n","        model.eval()\n","        for idx, (label, text) in enumerate(dataloader_val):\n","            logits = model(text)\n","            loss = criterion(logits, label)\n","            total_loss_val += loss\n","            total_acc_val += (logits.argmax(1) == label).sum().item()\n","            total_count_val += label.size(0)\n","            n_val_batches += 1\n","    avg_loss_val = total_loss_val/n_val_batches\n","    loss_val.append(avg_loss_val.item())\n","    accuracy_val = total_acc_val/total_count_val\n","    acc_val.append(accuracy_val) \n","    if epoch % 1 == 0:\n","      print(f\"epoch: {epoch+1} -> Accuracy: {100*accuracy_train:.2f}%, Loss: {avg_loss_train:.8f}\",end=\" ---------------- \")\n","      print(f\"Val_Acc: {100*accuracy_val:.2f}%, Val_Loss: {avg_loss_val:.8f}\")\n","  return loss_train, acc_train, loss_val, acc_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7L9U9KUySQVI"},"outputs":[],"source":["EPOCHS = 40\n","lr = 1e-4\n","overfit_model = FeedforwardNetwork(vocab_size, num_class, hidden_layers_dim=[256, 128, 64]).to(device)\n","optimizer = torch.optim.Adam(overfit_model.parameters(), lr=lr)\n","summary(overfit_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DLcj3cf5Sl1N"},"outputs":[],"source":["start = timer()\n","loss_train, accuracy_train, loss_val, accuracy_val = train_L2(overfit_model, optimizer, dataloader_training, dataloader_validation, l2_lambda=0.01)\n","end = timer()\n","print(f\"Training time in second: {(end - start)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pEtN-XdKsaFc"},"outputs":[],"source":["plot_learning_acc_and_loss(loss_train, accuracy_train, loss_val, accuracy_val)"]},{"cell_type":"markdown","source":["`%ENDCODE`"],"metadata":{"id":"1xwD7uY1GxV1"}},{"cell_type":"markdown","metadata":{"id":"CLTJUqX7VhuV"},"source":["# Exercise 2.4: Early Stopping\n","Early Stopping is a form of regularization used to avoid overfitting. It is designed to monitor the generalization error of one model and stop training when generalization error begins to degrade. In order to evaluate the generalization error, early stopping requires that a validation dataset is evaluated during training. Then, when the validation error does not improve for a specific number of epochs (a.k.a. the \"patience\" or \"tolerance\" hyper-parameter), it stops the training phase.\n","\n","To implement it in PyTorch, we define a simple classes that does this job."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kVPRIb1gQyoo"},"outputs":[],"source":["class EarlyStopping:\n","    def __init__(self, tolerance, min_delta):\n","        self.tolerance = tolerance\n","        self.min_delta = min_delta\n","        self.counter = 0\n","        self.early_stop = False\n","\n","    def __call__(self, train_loss, validation_loss):\n","        if (validation_loss - train_loss) > self.min_delta:\n","            self.counter +=1\n","            if self.counter >= self.tolerance:  \n","                self.early_stop = True"]},{"cell_type":"markdown","source":["## **[TO COMPLETE]**: Define training procedure\n","Define a training procedure with Early Stopping and check its performances.\n"],"metadata":{"id":"jeLHARUEHWsE"}},{"cell_type":"markdown","source":["\n","`%STARTCODE`"],"metadata":{"id":"PkzbQks8RFU6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LF2WyxJzWVZZ"},"outputs":[],"source":["def train_early_stop(model, optimizer, dataloader_train, dataloader_val, EPOCHS=EPOCHS):\n","  ###########################################################\n","  # instantiate an object of the EarlyStopping class \n","  #[TO COMPLETE]\n","  ###########################################################\n","  loss_train, loss_val = [], []\n","  acc_train, acc_val = [], []\n","  for epoch in range(EPOCHS):\n","\n","    model.train()\n","    total_acc_train, total_count_train, n_train_batches, total_loss_train = 0, 0, 0, 0\n","    for idx, (label, text) in enumerate(dataloader_train):\n","      optimizer.zero_grad()\n","      logits = model(text)\n","      loss = criterion(logits, label)\n","      total_loss_train += loss\n","      loss.backward()\n","      optimizer.step()\n","\n","      total_acc_train += (logits.argmax(1) == label).sum().item()\n","      total_count_train += label.size(0)\n","      n_train_batches += 1\n","\n","    avg_loss_train = total_loss_train/n_train_batches\n","    loss_train.append(avg_loss_train.item())\n","    accuracy_train = total_acc_train/total_count_train\n","    acc_train.append(accuracy_train)\n","    \n","    total_acc_val, total_count_val, n_val_batches, total_loss_val = 0, 0, 0, 0\n","    with torch.no_grad():\n","        model.eval()\n","        for idx, (label, text) in enumerate(dataloader_val):\n","            logits = model(text)\n","            loss = criterion(logits, label)\n","            total_loss_val += loss\n","            total_acc_val += (logits.argmax(1) == label).sum().item()\n","            total_count_val += label.size(0)\n","            n_val_batches += 1\n","    avg_loss_val = total_loss_val/n_val_batches\n","    loss_val.append(avg_loss_val.item())\n","    accuracy_val = total_acc_val/total_count_val\n","    acc_val.append(accuracy_val) \n","    if epoch % 1 == 0:\n","      print(f\"epoch: {epoch+1} -> Accuracy: {100*accuracy_train:.2f}%, Loss: {avg_loss_train:.8f}\",end=\" ---------------- \")\n","      print(f\"Val_Acc: {100*accuracy_val:.2f}%, Val_Loss: {avg_loss_val:.8f}\")\n","    ############################################################\n","    # Use the EarlyStopping object\n","    #[TO COMPLETE]\n","    ############################################################\n","  return loss_train, acc_train, loss_val, acc_val"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BUgQIJvBXcLP"},"outputs":[],"source":["EPOCHS = 40\n","lr = 1e-4\n","overfit_model = FeedforwardNetwork(vocab_size, num_class, hidden_layers_dim=[256, 128, 64]).to(device)\n","optimizer = torch.optim.Adam(overfit_model.parameters(), lr=lr)\n","summary(overfit_model)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gybj6eSEXowL"},"outputs":[],"source":["start = timer()\n","loss_train, accuracy_train, loss_val, accuracy_val = train_early_stop(overfit_model, optimizer, dataloader_training, dataloader_validation)\n","end = timer()\n","print(f\"Training time in second: {(end - start)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TvYcxe40tRD8"},"outputs":[],"source":["plot_learning_acc_and_loss(loss_train, accuracy_train, loss_val, accuracy_val)"]},{"cell_type":"markdown","source":["`%ENDCODE`"],"metadata":{"id":"auc9q8FWHmT6"}},{"cell_type":"markdown","metadata":{"id":"fpE49B6UVj1C"},"source":["# Model Selection\n","Hyperparameters are the parameters of the learning method itself which we have to specify a priori, i.e., before model fitting. In contrast, model parameters are parameters which arise as a result of the fit (the network weights). The aim of model selection is selecting the best hyperparameters for our deep network. Finding the right hyperparameters for a model can be crucial for the model performance on given data. For istance lets consider our model trained by using different values for the learning rate: "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lIpH-DSvVmy1"},"outputs":[],"source":["learning_rates = [1e-5, 1e-4, 1e-3]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EjnDps4lZRjr"},"outputs":[],"source":["stack = []\n","for lr in learning_rates:\n","  print(f\"Learning Rate: {lr}\")\n","  model = FeedforwardNetwork(vocab_size, num_class, hidden_layers_dim=[128, 64]).to(device)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","  stack.append(train(model, optimizer, dataloader_training, dataloader_validation, EPOCHS=15))\n","  print(\"-\"*50)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lh6Gp4MvbASS"},"outputs":[],"source":["# Let's plot the results\n","for idx, el in enumerate(stack):\n","  print(f\"Learning Rate: {learning_rates[idx]}\")\n","  plot_learning_acc_and_loss(*el)"]},{"cell_type":"markdown","metadata":{"id":"itlx4eMGVwhF"},"source":["# Exercise 2.5: Grid Search\n","\n","Since a deep net has many hyperparameters, in order to find the best ones, we have to consider different possible combinations of values for each hyper-parameter. One common method to perform this complex task is Grid-Search.\n","Given a set of values for each hyper-parameter, Grid-Search will build a model on each parameter combination possible. It iterates through every parameter combination and stores a model for each combination. Finally, the model that obtained the best result on the validation set will be selected.\n","\n","In order to perfrom Grid-Search we will use the `GridSearchCV` class from `scikit-learn`, together with some classes from the `skorch` package which allows to use PyTorch models with `scikit-learn`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"moDGnfYBVy0Y"},"outputs":[],"source":["model = NeuralNetClassifier(\n","    module=FeedforwardNetwork,\n","    criterion=torch.nn.CrossEntropyLoss,\n","    optimizer=torch.optim.Adam,\n","    max_epochs=2)\n","\n","# we need to adapt PyTorch Dataset to work with Scikit-Learn GridSearchCV\n","# we use the SliceDataset class from skorch package for this purpose\n","X_slice = SliceDataset(train_dataset, idx=1)\n","y_slice = SliceDataset(train_dataset, idx=0)"]},{"cell_type":"markdown","source":["## **[TO COMPLETE]**: Define hyperparameters grid"],"metadata":{"id":"gvZ3Reb4FwOX"}},{"cell_type":"markdown","source":["Let's define the lists of hyper-parameters' values. Also in this case, we use dictionaries of very limited size and hyperparameters, but in a real-world scenario a reasonable amount of possible values should be considered (and there are many other approacehs besisdes Grid-Search)."],"metadata":{"id":"pXOA474unH3P"}},{"cell_type":"markdown","source":["`%STARTCODE`"],"metadata":{"id":"wcDKGkbERR4u"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"DcSIrBz2eZLI"},"outputs":[],"source":["params = {\n","    'lr': #[TO COMPLETE],\n","    'module__input_dim': #[TO COMPLETE],\n","    'module__num_classes': #[TO COMPLETE],\n","    'module__hidden_layers_dim': #[TO COMPLETE], \n","    }"]},{"cell_type":"markdown","source":["`%ENDCODE`"],"metadata":{"id":"CppvMDaeI2PX"}},{"cell_type":"markdown","source":["Now you can use `sklearn`'s `GridSearchCV` to search the hyperparameter space of the `NeuralNetClassifier`:"],"metadata":{"id":"DOrBbzWVnX6Z"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"rsK_gOiPekcG"},"outputs":[],"source":["n_jobs = multiprocessing.cpu_count()-1\n","gs = GridSearchCV(model, params, n_jobs=n_jobs)"]},{"cell_type":"code","source":["outputs = gs.fit(X_slice, y_slice)"],"metadata":{"id":"nIhXvWonjZav"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **[TO COMPLETE]**: Visualize results\n","Print the best hyper-parameters and the performances on the best model on the test set:\n","\n"],"metadata":{"id":"Fr-SpSE7nyrl"}},{"cell_type":"markdown","source":["`%STARTCODE`"],"metadata":{"id":"zLCkmvC7RaI7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"wF9x4VoniCYm"},"outputs":[],"source":["print(\"best score: {:.3f}, best params: {}\".format()) #[TO COMPLETE]"]},{"cell_type":"code","source":["# Test the model\n","#[TO COMPLETE]"],"metadata":{"id":"vcPOwXsqGc2X"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["`%ENDCODE`"],"metadata":{"id":"ICBmyrHwJAix"}}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}